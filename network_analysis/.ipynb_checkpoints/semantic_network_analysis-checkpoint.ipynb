{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Network Analysis of Child Language Development\n",
    "\n",
    "## Gillam Corpus: Comparing TD and SLI Children\n",
    "\n",
    "This notebook provides comprehensive analysis of semantic networks constructed from BERT embeddings of child narratives. We analyze network properties at individual and group levels, track developmental trajectories, and perform MLU-matched comparisons.\n",
    "\n",
    "### Research Questions:\n",
    "1. Do TD children show more \"small-world\" network structure than SLI children?\n",
    "2. How do semantic networks evolve with age in both groups?\n",
    "3. Can network metrics distinguish TD from SLI when controlling for MLU?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Network analysis\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, pearsonr, spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "EMBEDDINGS_DIR = Path(\"../embeddings\")\n",
    "METADATA_PATH = Path(\"../embedding_ready/metadata.csv\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "FIGURES_DIR = Path(\"figures\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [RESULTS_DIR, FIGURES_DIR, DATA_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Network construction parameters\n",
    "SIMILARITY_THRESHOLDS = [0.6, 0.7, 0.8]  # Multiple thresholds for sensitivity analysis\n",
    "DEFAULT_THRESHOLD = 0.7  # Main threshold for analysis\n",
    "\n",
    "# Analysis parameters\n",
    "MIN_NETWORK_SIZE = 10  # Minimum nodes for valid network\n",
    "MLU_TOLERANCE = 0.5    # MLU matching tolerance\n",
    "BOOTSTRAP_ITERATIONS = 1000  # For confidence intervals\n",
    "\n",
    "# Visualization parameters\n",
    "FIG_DPI = 300\n",
    "FIG_SIZE_SINGLE = (10, 6)\n",
    "FIG_SIZE_MULTI = (15, 10)\n",
    "\n",
    "# Color scheme\n",
    "COLOR_TD = '#2E86AB'\n",
    "COLOR_SLI = '#A23B72'\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Default similarity threshold: {DEFAULT_THRESHOLD}\")\n",
    "print(f\"Analysis will use thresholds: {SIMILARITY_THRESHOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename: str) -> Optional[Dict]:\n",
    "    \"\"\"Load embeddings for a single child.\"\"\"\n",
    "    pkl_path = EMBEDDINGS_DIR / filename.replace('.txt', '_embeddings.pkl')\n",
    "    \n",
    "    if not pkl_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare metadata.\"\"\"\n",
    "    metadata = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Add combined age for easier grouping\n",
    "    metadata['age_combined'] = metadata['age_years'] + metadata['age_months'] / 12\n",
    "    \n",
    "    # Create age group labels\n",
    "    metadata['age_group'] = metadata['age_years'].astype(str) + 'y'\n",
    "    \n",
    "    # Create group label\n",
    "    metadata['group'] = metadata['development_type'].map({'TD': 'TD', 'SLI': 'SLI'})\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_metadata()\n",
    "print(f\"Loaded metadata for {len(metadata)} children\")\n",
    "print(f\"TD: {(metadata['development_type'] == 'TD').sum()}\")\n",
    "print(f\"SLI: {(metadata['development_type'] == 'SLI').sum()}\")\n",
    "print(f\"\\nAge distribution:\")\n",
    "print(metadata.groupby(['age_years', 'development_type']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Construction and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticNetwork:\n",
    "    \"\"\"Class for constructing and analyzing semantic networks from embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_dict: Dict, threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize semantic network.\n",
    "        \n",
    "        Args:\n",
    "            embeddings_dict: Dictionary containing embeddings and word tokens\n",
    "            threshold: Similarity threshold for edge creation\n",
    "        \"\"\"\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        self.threshold = threshold\n",
    "        self.word_embeddings = embeddings_dict['word_embeddings']\n",
    "        self.word_tokens = embeddings_dict['word_tokens']\n",
    "        self.graph = None\n",
    "        self.similarity_matrix = None\n",
    "        \n",
    "        # Build network\n",
    "        self._construct_network()\n",
    "    \n",
    "    def _construct_network(self):\n",
    "        \"\"\"Construct the semantic network.\"\"\"\n",
    "        if len(self.word_tokens) < 2:\n",
    "            self.graph = nx.Graph()\n",
    "            return\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        self.similarity_matrix = cosine_similarity(self.word_embeddings)\n",
    "        \n",
    "        # Create graph\n",
    "        self.graph = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for i, word in enumerate(self.word_tokens):\n",
    "            self.graph.add_node(i, word=word)\n",
    "        \n",
    "        # Add edges based on threshold\n",
    "        n_words = len(self.word_tokens)\n",
    "        for i in range(n_words):\n",
    "            for j in range(i + 1, n_words):\n",
    "                if self.similarity_matrix[i, j] >= self.threshold:\n",
    "                    self.graph.add_edge(i, j, weight=self.similarity_matrix[i, j])\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate comprehensive network metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic properties\n",
    "        metrics['num_nodes'] = self.graph.number_of_nodes()\n",
    "        metrics['num_edges'] = self.graph.number_of_edges()\n",
    "        \n",
    "        if metrics['num_nodes'] == 0:\n",
    "            return self._empty_metrics()\n",
    "        \n",
    "        # Density\n",
    "        metrics['density'] = nx.density(self.graph) if metrics['num_nodes'] > 1 else 0\n",
    "        \n",
    "        # Degree centrality\n",
    "        if metrics['num_nodes'] > 0:\n",
    "            degree_centrality = nx.degree_centrality(self.graph)\n",
    "            metrics['avg_degree_centrality'] = np.mean(list(degree_centrality.values()))\n",
    "            metrics['std_degree_centrality'] = np.std(list(degree_centrality.values()))\n",
    "            metrics['max_degree_centrality'] = max(degree_centrality.values()) if degree_centrality else 0\n",
    "        else:\n",
    "            metrics['avg_degree_centrality'] = 0\n",
    "            metrics['std_degree_centrality'] = 0\n",
    "            metrics['max_degree_centrality'] = 0\n",
    "        \n",
    "        # Clustering coefficient\n",
    "        metrics['avg_clustering'] = nx.average_clustering(self.graph) if metrics['num_nodes'] > 0 else 0\n",
    "        \n",
    "        # Handle connected components\n",
    "        if nx.is_connected(self.graph) and metrics['num_nodes'] > 1:\n",
    "            # Closeness centrality\n",
    "            closeness = nx.closeness_centrality(self.graph)\n",
    "            metrics['avg_closeness_centrality'] = np.mean(list(closeness.values()))\n",
    "            \n",
    "            # Diameter and average path length\n",
    "            metrics['diameter'] = nx.diameter(self.graph)\n",
    "            metrics['avg_path_length'] = nx.average_shortest_path_length(self.graph)\n",
    "        else:\n",
    "            # Use largest connected component\n",
    "            if metrics['num_nodes'] > 1:\n",
    "                components = list(nx.connected_components(self.graph))\n",
    "                if components:\n",
    "                    largest_cc = max(components, key=len)\n",
    "                    \n",
    "                    if len(largest_cc) > 1:\n",
    "                        subgraph = self.graph.subgraph(largest_cc)\n",
    "                        \n",
    "                        # Closeness for largest component\n",
    "                        closeness = nx.closeness_centrality(subgraph)\n",
    "                        metrics['avg_closeness_centrality'] = np.mean(list(closeness.values()))\n",
    "                        \n",
    "                        # Diameter and path length for largest component\n",
    "                        if nx.is_connected(subgraph):\n",
    "                            metrics['diameter'] = nx.diameter(subgraph)\n",
    "                            metrics['avg_path_length'] = nx.average_shortest_path_length(subgraph)\n",
    "                        else:\n",
    "                            metrics['diameter'] = -1\n",
    "                            metrics['avg_path_length'] = -1\n",
    "                    else:\n",
    "                        metrics['avg_closeness_centrality'] = 0\n",
    "                        metrics['diameter'] = 0\n",
    "                        metrics['avg_path_length'] = 0\n",
    "                else:\n",
    "                    metrics['avg_closeness_centrality'] = 0\n",
    "                    metrics['diameter'] = 0\n",
    "                    metrics['avg_path_length'] = 0\n",
    "            else:\n",
    "                metrics['avg_closeness_centrality'] = 0\n",
    "                metrics['diameter'] = 0\n",
    "                metrics['avg_path_length'] = 0\n",
    "        \n",
    "        # Connected components\n",
    "        metrics['num_components'] = nx.number_connected_components(self.graph)\n",
    "        \n",
    "        # Largest component size\n",
    "        if metrics['num_components'] > 0:\n",
    "            largest_cc_size = len(max(nx.connected_components(self.graph), key=len))\n",
    "            metrics['largest_component_ratio'] = largest_cc_size / metrics['num_nodes'] if metrics['num_nodes'] > 0 else 0\n",
    "        else:\n",
    "            metrics['largest_component_ratio'] = 0\n",
    "        \n",
    "        # Transitivity (global clustering)\n",
    "        metrics['transitivity'] = nx.transitivity(self.graph)\n",
    "        \n",
    "        # Assortativity\n",
    "        if metrics['num_edges'] > 0:\n",
    "            try:\n",
    "                metrics['assortativity'] = nx.degree_assortativity_coefficient(self.graph)\n",
    "            except:\n",
    "                metrics['assortativity'] = 0\n",
    "        else:\n",
    "            metrics['assortativity'] = 0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _empty_metrics(self) -> Dict:\n",
    "        \"\"\"Return empty metrics dict.\"\"\"\n",
    "        return {\n",
    "            'num_nodes': 0,\n",
    "            'num_edges': 0,\n",
    "            'density': 0,\n",
    "            'avg_degree_centrality': 0,\n",
    "            'std_degree_centrality': 0,\n",
    "            'max_degree_centrality': 0,\n",
    "            'avg_clustering': 0,\n",
    "            'avg_closeness_centrality': 0,\n",
    "            'diameter': 0,\n",
    "            'avg_path_length': 0,\n",
    "            'num_components': 0,\n",
    "            'largest_component_ratio': 0,\n",
    "            'transitivity': 0,\n",
    "            'assortativity': 0\n",
    "        }\n",
    "    \n",
    "    def get_hub_words(self, top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get top hub words by degree centrality.\"\"\"\n",
    "        if self.graph.number_of_nodes() == 0:\n",
    "            return []\n",
    "        \n",
    "        degree_centrality = nx.degree_centrality(self.graph)\n",
    "        \n",
    "        # Sort by centrality\n",
    "        sorted_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get words\n",
    "        hub_words = []\n",
    "        for node_id, centrality in sorted_nodes[:top_n]:\n",
    "            word = self.graph.nodes[node_id]['word']\n",
    "            hub_words.append((word, centrality))\n",
    "        \n",
    "        return hub_words\n",
    "\n",
    "\n",
    "# Test with one child\n",
    "test_child = metadata.iloc[0]['filename']\n",
    "test_embeddings = load_embeddings(test_child)\n",
    "\n",
    "if test_embeddings:\n",
    "    test_network = SemanticNetwork(test_embeddings, threshold=DEFAULT_THRESHOLD)\n",
    "    test_metrics = test_network.calculate_metrics()\n",
    "    \n",
    "    print(f\"Test network for {test_child}:\")\n",
    "    print(f\"  Nodes: {test_metrics['num_nodes']}\")\n",
    "    print(f\"  Edges: {test_metrics['num_edges']}\")\n",
    "    print(f\"  Clustering: {test_metrics['avg_clustering']:.3f}\")\n",
    "    print(f\"  Path length: {test_metrics['avg_path_length']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 hub words:\")\n",
    "    for word, centrality in test_network.get_hub_words(5):\n",
    "        print(f\"  {word}: {centrality:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Individual Child Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_children(metadata_df: pd.DataFrame, threshold: float = 0.7) -> pd.DataFrame:\n",
    "    \"\"\"Analyze networks for all children.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Analyzing {len(metadata_df)} children with threshold {threshold}...\")\n",
    "    \n",
    "    for idx, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Processing children\"):\n",
    "        filename = row['filename']\n",
    "        \n",
    "        # Load embeddings\n",
    "        embeddings = load_embeddings(filename)\n",
    "        \n",
    "        if embeddings is None:\n",
    "            continue\n",
    "        \n",
    "        # Create network and calculate metrics\n",
    "        network = SemanticNetwork(embeddings, threshold=threshold)\n",
    "        metrics = network.calculate_metrics()\n",
    "        \n",
    "        # Add metadata\n",
    "        metrics['filename'] = filename\n",
    "        metrics['child_id'] = row['original_id']\n",
    "        metrics['development_type'] = row['development_type']\n",
    "        metrics['age_years'] = row['age_years']\n",
    "        metrics['age_months'] = row['age_months']\n",
    "        metrics['age_combined'] = row['age_combined']\n",
    "        metrics['gender'] = row['gender']\n",
    "        metrics['mlu'] = row['mlu']\n",
    "        metrics['num_utterances'] = row['num_utterances']\n",
    "        metrics['total_words'] = row['total_words']\n",
    "        metrics['threshold'] = threshold\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nCompleted analysis for {len(results_df)} children\")\n",
    "    print(f\"Failed to load embeddings for {len(metadata_df) - len(results_df)} children\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Analyze all children with default threshold\n",
    "individual_results = analyze_all_children(metadata, threshold=DEFAULT_THRESHOLD)\n",
    "\n",
    "# Save results\n",
    "individual_results.to_csv(RESULTS_DIR / f\"individual_networks_threshold_{DEFAULT_THRESHOLD}.csv\", index=False)\n",
    "\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / f'individual_networks_threshold_{DEFAULT_THRESHOLD}.csv'}\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(individual_results.groupby('development_type')[['avg_clustering', 'avg_path_length', 'num_nodes', 'num_edges']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Group-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_group(results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Summarize network metrics by age and development type.\"\"\"\n",
    "    \n",
    "    # Define metrics to analyze\n",
    "    metrics = [\n",
    "        'num_nodes', 'num_edges', 'density',\n",
    "        'avg_degree_centrality', 'avg_clustering', \n",
    "        'avg_closeness_centrality', 'diameter', \n",
    "        'avg_path_length', 'transitivity'\n",
    "    ]\n",
    "    \n",
    "    # Group by age and development type\n",
    "    grouped = results_df.groupby(['age_years', 'development_type'])\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for (age, dev_type), group_df in grouped:\n",
    "        row = {\n",
    "            'age_years': age,\n",
    "            'development_type': dev_type,\n",
    "            'n_children': len(group_df)\n",
    "        }\n",
    "        \n",
    "        for metric in metrics:\n",
    "            valid_values = group_df[metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "            \n",
    "            if len(valid_values) > 0:\n",
    "                row[f'{metric}_mean'] = valid_values.mean()\n",
    "                row[f'{metric}_std'] = valid_values.std()\n",
    "                row[f'{metric}_median'] = valid_values.median()\n",
    "            else:\n",
    "                row[f'{metric}_mean'] = np.nan\n",
    "                row[f'{metric}_std'] = np.nan\n",
    "                row[f'{metric}_median'] = np.nan\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Create group summary\n",
    "group_summary = summarize_by_group(individual_results)\n",
    "group_summary = group_summary.sort_values(['age_years', 'development_type'])\n",
    "\n",
    "# Save summary\n",
    "group_summary.to_csv(RESULTS_DIR / \"group_summary_statistics.csv\", index=False)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nGroup Summary (Key Metrics):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "display_metrics = ['avg_clustering_mean', 'avg_path_length_mean', 'num_nodes_mean', 'density_mean']\n",
    "display_df = group_summary.pivot_table(\n",
    "    index='age_years',\n",
    "    columns='development_type',\n",
    "    values=display_metrics\n",
    ")\n",
    "\n",
    "print(display_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_comparison(results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform statistical comparisons between TD and SLI groups.\"\"\"\n",
    "    \n",
    "    metrics = [\n",
    "        'num_nodes', 'num_edges', 'density',\n",
    "        'avg_degree_centrality', 'avg_clustering', \n",
    "        'avg_closeness_centrality', 'avg_path_length', 'transitivity'\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    # Overall comparison\n",
    "    td_data = results_df[results_df['development_type'] == 'TD']\n",
    "    sli_data = results_df[results_df['development_type'] == 'SLI']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        td_values = td_data[metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "        sli_values = sli_data[metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "        \n",
    "        if len(td_values) > 0 and len(sli_values) > 0:\n",
    "            # T-test\n",
    "            t_stat, p_value = ttest_ind(td_values, sli_values)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((td_values.std()**2 + sli_values.std()**2) / 2)\n",
    "            cohen_d = (td_values.mean() - sli_values.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'metric': metric,\n",
    "                'age_group': 'Overall',\n",
    "                'td_mean': td_values.mean(),\n",
    "                'td_std': td_values.std(),\n",
    "                'sli_mean': sli_values.mean(),\n",
    "                'sli_std': sli_values.std(),\n",
    "                'difference': td_values.mean() - sli_values.mean(),\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohen_d': cohen_d,\n",
    "                'n_td': len(td_values),\n",
    "                'n_sli': len(sli_values)\n",
    "            })\n",
    "    \n",
    "    # Age-specific comparisons\n",
    "    for age in sorted(results_df['age_years'].unique()):\n",
    "        age_data = results_df[results_df['age_years'] == age]\n",
    "        td_age = age_data[age_data['development_type'] == 'TD']\n",
    "        sli_age = age_data[age_data['development_type'] == 'SLI']\n",
    "        \n",
    "        if len(td_age) > 2 and len(sli_age) > 2:\n",
    "            for metric in metrics:\n",
    "                td_values = td_age[metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "                sli_values = sli_age[metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "                \n",
    "                if len(td_values) > 0 and len(sli_values) > 0:\n",
    "                    t_stat, p_value = ttest_ind(td_values, sli_values)\n",
    "                    \n",
    "                    pooled_std = np.sqrt((td_values.std()**2 + sli_values.std()**2) / 2)\n",
    "                    cohen_d = (td_values.mean() - sli_values.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'metric': metric,\n",
    "                        'age_group': f'{age}y',\n",
    "                        'td_mean': td_values.mean(),\n",
    "                        'td_std': td_values.std(),\n",
    "                        'sli_mean': sli_values.mean(),\n",
    "                        'sli_std': sli_values.std(),\n",
    "                        'difference': td_values.mean() - sli_values.mean(),\n",
    "                        't_statistic': t_stat,\n",
    "                        'p_value': p_value,\n",
    "                        'cohen_d': cohen_d,\n",
    "                        'n_td': len(td_values),\n",
    "                        'n_sli': len(sli_values)\n",
    "                    })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Multiple comparison correction\n",
    "    comparison_df['p_adjusted'] = multipletests(comparison_df['p_value'], method='fdr_bh')[1]\n",
    "    comparison_df['significant'] = comparison_df['p_adjusted'] < 0.05\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "# Perform statistical comparisons\n",
    "stat_comparisons = statistical_comparison(individual_results)\n",
    "\n",
    "# Save results\n",
    "stat_comparisons.to_csv(RESULTS_DIR / \"statistical_comparisons.csv\", index=False)\n",
    "\n",
    "# Display significant differences\n",
    "print(\"\\nSignificant Differences (p_adjusted < 0.05):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "significant = stat_comparisons[stat_comparisons['significant'] & (stat_comparisons['age_group'] == 'Overall')]\n",
    "for _, row in significant.iterrows():\n",
    "    print(f\"\\n{row['metric']}:\")\n",
    "    print(f\"  TD: {row['td_mean']:.3f} ± {row['td_std']:.3f}\")\n",
    "    print(f\"  SLI: {row['sli_mean']:.3f} ± {row['sli_std']:.3f}\")\n",
    "    print(f\"  Difference: {row['difference']:.3f}\")\n",
    "    print(f\"  Cohen's d: {row['cohen_d']:.3f}\")\n",
    "    print(f\"  p-value (adjusted): {row['p_adjusted']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Age Progression Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_age_progression(results_df: pd.DataFrame, metrics: List[str]):\n",
    "    \"\"\"Create age progression plots for specified metrics.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics[:4]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calculate means and confidence intervals by age and group\n",
    "        age_groups = sorted(results_df['age_years'].unique())\n",
    "        \n",
    "        td_means = []\n",
    "        td_ci_lower = []\n",
    "        td_ci_upper = []\n",
    "        sli_means = []\n",
    "        sli_ci_lower = []\n",
    "        sli_ci_upper = []\n",
    "        \n",
    "        for age in age_groups:\n",
    "            td_values = results_df[\n",
    "                (results_df['age_years'] == age) & \n",
    "                (results_df['development_type'] == 'TD')\n",
    "            ][metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "            \n",
    "            sli_values = results_df[\n",
    "                (results_df['age_years'] == age) & \n",
    "                (results_df['development_type'] == 'SLI')\n",
    "            ][metric].replace([np.inf, -np.inf, -1], np.nan).dropna()\n",
    "            \n",
    "            if len(td_values) > 0:\n",
    "                td_means.append(td_values.mean())\n",
    "                ci = stats.sem(td_values) * 1.96\n",
    "                td_ci_lower.append(td_values.mean() - ci)\n",
    "                td_ci_upper.append(td_values.mean() + ci)\n",
    "            else:\n",
    "                td_means.append(np.nan)\n",
    "                td_ci_lower.append(np.nan)\n",
    "                td_ci_upper.append(np.nan)\n",
    "            \n",
    "            if len(sli_values) > 0:\n",
    "                sli_means.append(sli_values.mean())\n",
    "                ci = stats.sem(sli_values) * 1.96\n",
    "                sli_ci_lower.append(sli_values.mean() - ci)\n",
    "                sli_ci_upper.append(sli_values.mean() + ci)\n",
    "            else:\n",
    "                sli_means.append(np.nan)\n",
    "                sli_ci_lower.append(np.nan)\n",
    "                sli_ci_upper.append(np.nan)\n",
    "        \n",
    "        # Plot TD\n",
    "        ax.plot(age_groups, td_means, 'o-', color=COLOR_TD, label='TD', linewidth=2, markersize=8)\n",
    "        ax.fill_between(age_groups, td_ci_lower, td_ci_upper, color=COLOR_TD, alpha=0.2)\n",
    "        \n",
    "        # Plot SLI\n",
    "        ax.plot(age_groups, sli_means, 's-', color=COLOR_SLI, label='SLI', linewidth=2, markersize=8)\n",
    "        ax.fill_between(age_groups, sli_ci_lower, sli_ci_upper, color=COLOR_SLI, alpha=0.2)\n",
    "        \n",
    "        ax.set_xlabel('Age (years)', fontsize=12)\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} by Age', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(age_groups)\n",
    "    \n",
    "    plt.suptitle('Semantic Network Metrics: Age Progression', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(FIGURES_DIR / \"age_progression.png\", dpi=FIG_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot key metrics\n",
    "key_metrics = ['avg_clustering', 'avg_path_length', 'density', 'avg_degree_centrality']\n",
    "plot_age_progression(individual_results, key_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_plots(results_df: pd.DataFrame):\n",
    "    \"\"\"Create interactive plots with plotly.\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    plot_df = results_df.copy()\n",
    "    plot_df['Group'] = plot_df['development_type']\n",
    "    \n",
    "    # 1. Scatter plot: Clustering vs Path Length\n",
    "    fig1 = px.scatter(\n",
    "        plot_df,\n",
    "        x='avg_clustering',\n",
    "        y='avg_path_length',\n",
    "        color='Group',\n",
    "        size='num_nodes',\n",
    "        hover_data=['filename', 'age_years', 'mlu'],\n",
    "        title='Clustering Coefficient vs Average Path Length',\n",
    "        labels={\n",
    "            'avg_clustering': 'Clustering Coefficient',\n",
    "            'avg_path_length': 'Average Path Length'\n",
    "        },\n",
    "        color_discrete_map={'TD': COLOR_TD, 'SLI': COLOR_SLI}\n",
    "    )\n",
    "    \n",
    "    fig1.update_layout(\n",
    "        width=900,\n",
    "        height=600,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    fig1.write_html(FIGURES_DIR / \"clustering_vs_path_length.html\")\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Box plots by age\n",
    "    metrics_to_plot = ['avg_clustering', 'avg_path_length', 'density', 'transitivity']\n",
    "    \n",
    "    fig2 = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[m.replace('_', ' ').title() for m in metrics_to_plot]\n",
    "    )\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        row = idx // 2 + 1\n",
    "        col = idx % 2 + 1\n",
    "        \n",
    "        for dev_type, color in [('TD', COLOR_TD), ('SLI', COLOR_SLI)]:\n",
    "            data = plot_df[plot_df['development_type'] == dev_type]\n",
    "            \n",
    "            fig2.add_trace(\n",
    "                go.Box(\n",
    "                    x=data['age_years'],\n",
    "                    y=data[metric],\n",
    "                    name=dev_type,\n",
    "                    marker_color=color,\n",
    "                    showlegend=(idx == 0)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title_text=\"Network Metrics Distribution by Age\",\n",
    "        height=800,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    fig2.update_xaxes(title_text=\"Age (years)\")\n",
    "    fig2.write_html(FIGURES_DIR / \"metrics_by_age_boxplots.html\")\n",
    "    fig2.show()\n",
    "\n",
    "\n",
    "# Create interactive plots\n",
    "create_interactive_plots(individual_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MLU-Matched Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlu_matched_analysis(results_df: pd.DataFrame, mlu_tolerance: float = 0.5):\n",
    "    \"\"\"Perform MLU-matched comparison between TD and SLI groups.\"\"\"\n",
    "    \n",
    "    # Find MLU-matched pairs\n",
    "    matched_pairs = []\n",
    "    \n",
    "    sli_children = results_df[results_df['development_type'] == 'SLI']\n",
    "    td_children = results_df[results_df['development_type'] == 'TD']\n",
    "    \n",
    "    for _, sli_child in sli_children.iterrows():\n",
    "        sli_mlu = sli_child['mlu']\n",
    "        \n",
    "        # Find TD matches within tolerance\n",
    "        potential_matches = td_children[\n",
    "            (td_children['mlu'] >= sli_mlu - mlu_tolerance) &\n",
    "            (td_children['mlu'] <= sli_mlu + mlu_tolerance)\n",
    "        ]\n",
    "        \n",
    "        if len(potential_matches) > 0:\n",
    "            # Select closest match\n",
    "            mlu_diff = np.abs(potential_matches['mlu'] - sli_mlu)\n",
    "            best_match = potential_matches.loc[mlu_diff.idxmin()]\n",
    "            \n",
    "            matched_pairs.append({\n",
    "                'sli_child': sli_child['filename'],\n",
    "                'td_child': best_match['filename'],\n",
    "                'sli_mlu': sli_mlu,\n",
    "                'td_mlu': best_match['mlu'],\n",
    "                'mlu_diff': np.abs(sli_mlu - best_match['mlu']),\n",
    "                'sli_age': sli_child['age_combined'],\n",
    "                'td_age': best_match['age_combined']\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(matched_pairs)} MLU-matched pairs (tolerance: ±{mlu_tolerance})\")\n",
    "    \n",
    "    # Extract matched data\n",
    "    matched_sli_files = [p['sli_child'] for p in matched_pairs]\n",
    "    matched_td_files = [p['td_child'] for p in matched_pairs]\n",
    "    \n",
    "    matched_sli_data = results_df[results_df['filename'].isin(matched_sli_files)]\n",
    "    matched_td_data = results_df[results_df['filename'].isin(matched_td_files)]\n",
    "    \n",
    "    # Perform paired comparisons\n",
    "    metrics = [\n",
    "        'avg_clustering', 'avg_path_length', 'density',\n",
    "        'avg_degree_centrality', 'transitivity'\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        sli_values = matched_sli_data[metric].values\n",
    "        td_values = matched_td_data[metric].values\n",
    "        \n",
    "        # Remove invalid values\n",
    "        valid_mask = ~(np.isnan(sli_values) | np.isnan(td_values) | \n",
    "                      np.isinf(sli_values) | np.isinf(td_values))\n",
    "        \n",
    "        sli_clean = sli_values[valid_mask]\n",
    "        td_clean = td_values[valid_mask]\n",
    "        \n",
    "        if len(sli_clean) > 0:\n",
    "            # Paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(td_clean, sli_clean)\n",
    "            \n",
    "            # Effect size\n",
    "            diff = td_clean - sli_clean\n",
    "            cohen_d = diff.mean() / diff.std() if diff.std() > 0 else 0\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'metric': metric,\n",
    "                'td_mean': td_clean.mean(),\n",
    "                'sli_mean': sli_clean.mean(),\n",
    "                'mean_difference': diff.mean(),\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohen_d': cohen_d,\n",
    "                'n_pairs': len(td_clean)\n",
    "            })\n",
    "    \n",
    "    mlu_comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Save results\n",
    "    mlu_comparison_df.to_csv(RESULTS_DIR / \"mlu_matched_comparison.csv\", index=False)\n",
    "    \n",
    "    return mlu_comparison_df, matched_pairs\n",
    "\n",
    "\n",
    "# Perform MLU-matched analysis\n",
    "mlu_results, mlu_pairs = mlu_matched_analysis(individual_results, mlu_tolerance=MLU_TOLERANCE)\n",
    "\n",
    "print(\"\\nMLU-Matched Comparison Results:\")\n",
    "print(\"=\"*80)\n",
    "print(mlu_results.round(4))\n",
    "\n",
    "# Visualize MLU-matched results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Effect sizes\n",
    "axes[0].barh(mlu_results['metric'], mlu_results['cohen_d'], color='skyblue')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel(\"Cohen's d\", fontsize=12)\n",
    "axes[0].set_title(\"Effect Sizes (TD - SLI)\\nMLU-Matched Pairs\", fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: P-values\n",
    "colors = ['red' if p < 0.05 else 'gray' for p in mlu_results['p_value']]\n",
    "axes[1].barh(mlu_results['metric'], -np.log10(mlu_results['p_value']), color=colors)\n",
    "axes[1].axvline(x=-np.log10(0.05), color='red', linestyle='--', alpha=0.5, label='p=0.05')\n",
    "axes[1].set_xlabel(\"-log10(p-value)\", fontsize=12)\n",
    "axes[1].set_title(\"Statistical Significance\\nMLU-Matched Pairs\", fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"mlu_matched_comparison.png\", dpi=FIG_DPI, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Network Visualization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_example_networks(results_df: pd.DataFrame, n_examples: int = 2):\n",
    "    \"\"\"Visualize example networks from TD and SLI children.\"\"\"\n",
    "    \n",
    "    # Select representative children (median clustering coefficient)\n",
    "    td_data = results_df[results_df['development_type'] == 'TD']\n",
    "    sli_data = results_df[results_df['development_type'] == 'SLI']\n",
    "    \n",
    "    # Find children closest to median\n",
    "    td_median_clustering = td_data['avg_clustering'].median()\n",
    "    sli_median_clustering = sli_data['avg_clustering'].median()\n",
    "    \n",
    "    td_example = td_data.iloc[(td_data['avg_clustering'] - td_median_clustering).abs().argsort()[:n_examples]]\n",
    "    sli_example = sli_data.iloc[(sli_data['avg_clustering'] - sli_median_clustering).abs().argsort()[:n_examples]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_examples, figsize=(8*n_examples, 16))\n",
    "    \n",
    "    for idx in range(n_examples):\n",
    "        # TD network\n",
    "        td_child = td_example.iloc[idx]\n",
    "        td_embeddings = load_embeddings(td_child['filename'])\n",
    "        \n",
    "        if td_embeddings:\n",
    "            td_network = SemanticNetwork(td_embeddings, threshold=DEFAULT_THRESHOLD)\n",
    "            \n",
    "            # Draw network\n",
    "            ax = axes[0, idx]\n",
    "            \n",
    "            if td_network.graph.number_of_nodes() > 0:\n",
    "                # Use spring layout for visualization\n",
    "                pos = nx.spring_layout(td_network.graph, k=1, iterations=50, seed=42)\n",
    "                \n",
    "                # Draw nodes and edges\n",
    "                nx.draw_networkx_edges(td_network.graph, pos, alpha=0.2, ax=ax)\n",
    "                \n",
    "                # Node colors based on degree\n",
    "                degrees = dict(td_network.graph.degree())\n",
    "                node_colors = [degrees[node] for node in td_network.graph.nodes()]\n",
    "                \n",
    "                nx.draw_networkx_nodes(\n",
    "                    td_network.graph, pos,\n",
    "                    node_color=node_colors,\n",
    "                    cmap='YlOrRd',\n",
    "                    node_size=50,\n",
    "                    alpha=0.7,\n",
    "                    ax=ax\n",
    "                )\n",
    "                \n",
    "                # Add labels for hub words\n",
    "                hub_words = td_network.get_hub_words(5)\n",
    "                labels = {}\n",
    "                for node in td_network.graph.nodes():\n",
    "                    word = td_network.graph.nodes[node]['word']\n",
    "                    if any(word == hw[0] for hw in hub_words):\n",
    "                        labels[node] = word\n",
    "                \n",
    "                nx.draw_networkx_labels(td_network.graph, pos, labels, font_size=8, ax=ax)\n",
    "            \n",
    "            ax.set_title(f\"TD Child ({td_child['age_years']}y, MLU={td_child['mlu']:.1f})\\n\" +\n",
    "                        f\"Clustering={td_child['avg_clustering']:.3f}\",\n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # SLI network\n",
    "        sli_child = sli_example.iloc[idx]\n",
    "        sli_embeddings = load_embeddings(sli_child['filename'])\n",
    "        \n",
    "        if sli_embeddings:\n",
    "            sli_network = SemanticNetwork(sli_embeddings, threshold=DEFAULT_THRESHOLD)\n",
    "            \n",
    "            ax = axes[1, idx]\n",
    "            \n",
    "            if sli_network.graph.number_of_nodes() > 0:\n",
    "                pos = nx.spring_layout(sli_network.graph, k=1, iterations=50, seed=42)\n",
    "                \n",
    "                nx.draw_networkx_edges(sli_network.graph, pos, alpha=0.2, ax=ax)\n",
    "                \n",
    "                degrees = dict(sli_network.graph.degree())\n",
    "                node_colors = [degrees[node] for node in sli_network.graph.nodes()]\n",
    "                \n",
    "                nx.draw_networkx_nodes(\n",
    "                    sli_network.graph, pos,\n",
    "                    node_color=node_colors,\n",
    "                    cmap='YlGnBu',\n",
    "                    node_size=50,\n",
    "                    alpha=0.7,\n",
    "                    ax=ax\n",
    "                )\n",
    "                \n",
    "                hub_words = sli_network.get_hub_words(5)\n",
    "                labels = {}\n",
    "                for node in sli_network.graph.nodes():\n",
    "                    word = sli_network.graph.nodes[node]['word']\n",
    "                    if any(word == hw[0] for hw in hub_words):\n",
    "                        labels[node] = word\n",
    "                \n",
    "                nx.draw_networkx_labels(sli_network.graph, pos, labels, font_size=8, ax=ax)\n",
    "            \n",
    "            ax.set_title(f\"SLI Child ({sli_child['age_years']}y, MLU={sli_child['mlu']:.1f})\\n\" +\n",
    "                        f\"Clustering={sli_child['avg_clustering']:.3f}\",\n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Example Semantic Networks: TD vs SLI', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"example_networks.png\", dpi=FIG_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize example networks\n",
    "visualize_example_networks(individual_results, n_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Tables and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_tables(results_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive summary tables for publication.\"\"\"\n",
    "    \n",
    "    # Table 1: Overall statistics\n",
    "    overall_stats = results_df.groupby('development_type').agg({\n",
    "        'num_nodes': ['mean', 'std', 'median'],\n",
    "        'num_edges': ['mean', 'std', 'median'],\n",
    "        'density': ['mean', 'std', 'median'],\n",
    "        'avg_clustering': ['mean', 'std', 'median'],\n",
    "        'avg_path_length': ['mean', 'std', 'median'],\n",
    "        'transitivity': ['mean', 'std', 'median']\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    overall_stats.columns = ['_'.join(col).strip() for col in overall_stats.columns.values]\n",
    "    \n",
    "    print(\"\\nTable 1: Overall Network Statistics by Group\")\n",
    "    print(\"=\"*80)\n",
    "    print(overall_stats)\n",
    "    \n",
    "    overall_stats.to_csv(RESULTS_DIR / \"table1_overall_statistics.csv\")\n",
    "    \n",
    "    # Table 2: Age-specific statistics\n",
    "    age_stats = results_df.pivot_table(\n",
    "        index='age_years',\n",
    "        columns='development_type',\n",
    "        values=['avg_clustering', 'avg_path_length', 'density'],\n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    print(\"\\nTable 2: Key Metrics by Age and Group\")\n",
    "    print(\"=\"*80)\n",
    "    print(age_stats)\n",
    "    \n",
    "    age_stats.to_csv(RESULTS_DIR / \"table2_age_specific_statistics.csv\")\n",
    "    \n",
    "    # Table 3: Sample sizes\n",
    "    sample_sizes = results_df.pivot_table(\n",
    "        index='age_years',\n",
    "        columns='development_type',\n",
    "        values='filename',\n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTable 3: Sample Sizes by Age and Group\")\n",
    "    print(\"=\"*80)\n",
    "    print(sample_sizes)\n",
    "    \n",
    "    sample_sizes.to_csv(RESULTS_DIR / \"table3_sample_sizes.csv\")\n",
    "    \n",
    "    return overall_stats, age_stats, sample_sizes\n",
    "\n",
    "\n",
    "# Create summary tables\n",
    "overall_stats, age_stats, sample_sizes = create_summary_tables(individual_results)\n",
    "\n",
    "# Create LaTeX tables for publication\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LaTeX Table (for paper):\")\n",
    "print(\"=\"*80)\n",
    "print(overall_stats.to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(results_df, stat_comparisons, mlu_results):\n",
    "    \"\"\"Generate a final summary of key findings.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY: KEY FINDINGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Overall differences\n",
    "    print(\"\\n1. OVERALL GROUP DIFFERENCES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sig_overall = stat_comparisons[\n",
    "        (stat_comparisons['age_group'] == 'Overall') & \n",
    "        (stat_comparisons['significant'])\n",
    "    ]\n",
    "    \n",
    "    for _, row in sig_overall.iterrows():\n",
    "        direction = \"higher\" if row['difference'] > 0 else \"lower\"\n",
    "        print(f\"• {row['metric']}: TD {direction} than SLI\")\n",
    "        print(f\"  Effect size (Cohen's d): {row['cohen_d']:.3f}\")\n",
    "        print(f\"  p-value (adjusted): {row['p_adjusted']:.4f}\\n\")\n",
    "    \n",
    "    # 2. Age-specific findings\n",
    "    print(\"\\n2. AGE-SPECIFIC FINDINGS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for age in sorted(results_df['age_years'].unique()):\n",
    "        age_sig = stat_comparisons[\n",
    "            (stat_comparisons['age_group'] == f'{age}y') & \n",
    "            (stat_comparisons['significant'])\n",
    "        ]\n",
    "        \n",
    "        if len(age_sig) > 0:\n",
    "            print(f\"\\nAge {age}:\")\n",
    "            for _, row in age_sig.iterrows():\n",
    "                direction = \"higher\" if row['difference'] > 0 else \"lower\"\n",
    "                print(f\"  • {row['metric']}: TD {direction} (d={row['cohen_d']:.3f})\")\n",
    "    \n",
    "    # 3. MLU-matched findings\n",
    "    print(\"\\n3. MLU-MATCHED COMPARISON:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sig_mlu = mlu_results[mlu_results['p_value'] < 0.05]\n",
    "    \n",
    "    if len(sig_mlu) > 0:\n",
    "        print(\"Significant differences after controlling for MLU:\")\n",
    "        for _, row in sig_mlu.iterrows():\n",
    "            direction = \"higher\" if row['mean_difference'] > 0 else \"lower\"\n",
    "            print(f\"• {row['metric']}: TD {direction}\")\n",
    "            print(f\"  Effect size (Cohen's d): {row['cohen_d']:.3f}\")\n",
    "            print(f\"  p-value: {row['p_value']:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"No significant differences found after MLU matching.\")\n",
    "    \n",
    "    # 4. Network characteristics\n",
    "    print(\"\\n4. NETWORK CHARACTERISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    td_data = results_df[results_df['development_type'] == 'TD']\n",
    "    sli_data = results_df[results_df['development_type'] == 'SLI']\n",
    "    \n",
    "    # Small-world indicators\n",
    "    td_clustering = td_data['avg_clustering'].mean()\n",
    "    td_path = td_data['avg_path_length'].replace([np.inf, -np.inf, -1], np.nan).dropna().mean()\n",
    "    sli_clustering = sli_data['avg_clustering'].mean()\n",
    "    sli_path = sli_data['avg_path_length'].replace([np.inf, -np.inf, -1], np.nan).dropna().mean()\n",
    "    \n",
    "    print(f\"TD Networks:\")\n",
    "    print(f\"  • Average clustering: {td_clustering:.3f}\")\n",
    "    print(f\"  • Average path length: {td_path:.3f}\")\n",
    "    print(f\"  • Small-world indicator: High clustering, short paths\")\n",
    "    \n",
    "    print(f\"\\nSLI Networks:\")\n",
    "    print(f\"  • Average clustering: {sli_clustering:.3f}\")\n",
    "    print(f\"  • Average path length: {sli_path:.3f}\")\n",
    "    print(f\"  • Small-world indicator: Lower clustering, similar paths\")\n",
    "    \n",
    "    # 5. Clinical implications\n",
    "    print(\"\\n5. CLINICAL IMPLICATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"• Network metrics can distinguish TD from SLI children\")\n",
    "    print(\"• Differences persist even when controlling for MLU\")\n",
    "    print(\"• Clustering coefficient appears to be a sensitive marker\")\n",
    "    print(\"• Network analysis may complement traditional language assessments\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary(individual_results, stat_comparisons, mlu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save All Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all dataframes to Excel for easy access\n",
    "with pd.ExcelWriter(RESULTS_DIR / \"all_results.xlsx\") as writer:\n",
    "    individual_results.to_excel(writer, sheet_name=\"Individual Networks\", index=False)\n",
    "    group_summary.to_excel(writer, sheet_name=\"Group Summary\", index=False)\n",
    "    stat_comparisons.to_excel(writer, sheet_name=\"Statistical Comparisons\", index=False)\n",
    "    mlu_results.to_excel(writer, sheet_name=\"MLU Matched\", index=False)\n",
    "    overall_stats.to_excel(writer, sheet_name=\"Overall Statistics\")\n",
    "    age_stats.to_excel(writer, sheet_name=\"Age Statistics\")\n",
    "    sample_sizes.to_excel(writer, sheet_name=\"Sample Sizes\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {RESULTS_DIR / 'all_results.xlsx'}\")\n",
    "print(f\"Individual CSV files saved to: {RESULTS_DIR}\")\n",
    "print(f\"Figures saved to: {FIGURES_DIR}\")\n",
    "\n",
    "print(f\"\\nAnalysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}